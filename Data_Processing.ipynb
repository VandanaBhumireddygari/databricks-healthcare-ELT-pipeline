{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11723612-1c42-46ea-a63c-85afce695197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Partner config (config-driven, required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97a12381-2ea4-486d-a1af-320bb157527c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks / PySpark ‚Äî Full end-to-end solution (Acme + BetterCare)\n",
    "# - Reads raw files from S3 (your external location already allows spark.read from s3://)\n",
    "# - Configuration-driven ingestion (delimiter + column mappings)\n",
    "# - Standardizes to unified schema + required transformations\n",
    "# - Writes Silver as Delta to S3\n",
    "# - Publishes Gold unified Delta table\n",
    "# - Writes Rejects (missing external_id) as Delta\n",
    "#\n",
    "# Matches assessment requirements. :contentReference[oaicite:0]{index=0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c33b2e4-5c61-4aa7-b109-ddaa26256a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =========================\n",
    "# 1) CONFIG (edit paths if needed)\n",
    "# =========================\n",
    "PARTNER_CONFIGS = {\n",
    "    \"acme\": {\n",
    "        \"partner_code\": \"ACME\",\n",
    "        \"path\": \"s3://databricks-age-bold/partner/acme/acme.txt\",\n",
    "        \"delimiter\": \"|\",\n",
    "        \"dob_format\": \"MM/dd/yyyy\",\n",
    "        \"column_mapping\": {\n",
    "            \"MBI\": \"external_id\",\n",
    "            \"FNAME\": \"first_name\",\n",
    "            \"LNAME\": \"last_name\",\n",
    "            \"DOB\": \"dob\",\n",
    "            \"EMAIL\": \"email\",\n",
    "            \"PHONE\": \"phone\"\n",
    "        }\n",
    "    },\n",
    "    \"bettercare\": {\n",
    "        \"partner_code\": \"BETTERCARE\",\n",
    "        \"path\": \"s3://databricks-age-bold/partner/bettercare/bettercare.csv\",\n",
    "        \"delimiter\": \",\",\n",
    "        \"dob_format\": \"yyyy-MM-dd\",\n",
    "        \"column_mapping\": {\n",
    "            \"subscriber_id\": \"external_id\",\n",
    "            \"first_name\": \"first_name\",\n",
    "            \"last_name\": \"last_name\",\n",
    "            \"date_of_birth\": \"dob\",\n",
    "            \"email\": \"email\",\n",
    "            \"phone\": \"phone\"\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe5147d7-6ae4-4449-9f93-4ba7764c15b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SILVER_PATH = \"s3://databricks-age-bold/silver/eligibility_delta\"\n",
    "REJECTS_PATH = \"s3://databricks-age-bold/rejects/eligibility_delta\"\n",
    "GOLD_TABLE = \"eligibility_gold_unified\"  # Delta table in metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c8e5b8b-bce9-4eaf-98eb-23bb398f365e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 2 ‚Äî Generic transformation function (core logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be32fcbe-561c-44e5-a320-febe1dd8bf37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "STD_COLS = [\"external_id\", \"first_name\", \"last_name\", \"dob\", \"email\", \"phone\", \"partner_code\"]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Helpers\n",
    "# =========================\n",
    "def format_phone(col):\n",
    "    \"\"\"\n",
    "    Normalize phone into XXX-XXX-XXXX if exactly 10 digits after stripping non-digits.\n",
    "    Otherwise set NULL.\n",
    "    \"\"\"\n",
    "    digits = F.regexp_replace(col.cast(\"string\"), r\"[^0-9]\", \"\")\n",
    "    return F.when(\n",
    "        F.length(digits) == 10,\n",
    "        F.concat_ws(\n",
    "            \"-\",\n",
    "            F.substring(digits, 1, 3),\n",
    "            F.substring(digits, 4, 3),\n",
    "            F.substring(digits, 7, 4),\n",
    "        )\n",
    "    ).otherwise(F.lit(None))\n",
    "\n",
    "\n",
    "def process_partner(cfg: dict):\n",
    "    \"\"\"\n",
    "    Read raw partner file, map to standard schema, apply required transformations,\n",
    "    and return (good_rows_df, bad_rows_df).\n",
    "    \"\"\"\n",
    "    # Read raw\n",
    "    df = (\n",
    "        spark.read\n",
    "             .option(\"header\", True)\n",
    "             .option(\"sep\", cfg[\"delimiter\"])\n",
    "             .option(\"mode\", \"PERMISSIVE\")\n",
    "             .csv(cfg[\"path\"])\n",
    "    )\n",
    "\n",
    "    # Rename -> standard schema using config mapping\n",
    "    for src, tgt in cfg[\"column_mapping\"].items():\n",
    "        if src in df.columns:\n",
    "            df = df.withColumnRenamed(src, tgt)\n",
    "\n",
    "    # Select only mapped standard columns (ignore any extras)\n",
    "    df = df.select(list(cfg[\"column_mapping\"].values()))\n",
    "\n",
    "    # Transformations required by assessment\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"first_name\", F.initcap(F.col(\"first_name\")))\n",
    "        .withColumn(\"last_name\", F.initcap(F.col(\"last_name\")))\n",
    "        .withColumn(\"email\", F.lower(F.col(\"email\")))\n",
    "        .withColumn(\"dob\", F.date_format(F.to_date(F.col(\"dob\").cast(\"string\"), cfg[\"dob_format\"]), \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"phone\", format_phone(F.col(\"phone\")))\n",
    "        .withColumn(\"partner_code\", F.lit(cfg[\"partner_code\"]))\n",
    "    ).select(STD_COLS)\n",
    "\n",
    "    # Bonus validation: external_id must be present\n",
    "    good = df.filter(F.col(\"external_id\").isNotNull() & (F.length(F.col(\"external_id\")) > 0))\n",
    "    bad = df.filter(F.col(\"external_id\").isNull() | (F.length(F.col(\"external_id\")) == 0))\n",
    "\n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be28362-db16-4cd9-ac19-74bc0246fcf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 3 ‚Äî Run Silver ingestion (standardized la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "454fbbc1-860b-4c85-a8b3-1410a8783542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 3) Run pipeline (Acme + BetterCare) -> Silver + Rejects\n",
    "# =========================\n",
    "silver_dfs = []\n",
    "total_good = 0\n",
    "total_bad = 0\n",
    "\n",
    "for partner_key, cfg in PARTNER_CONFIGS.items():\n",
    "    print(f\"Processing partner: {partner_key} ({cfg['partner_code']}) from {cfg['path']}\")\n",
    "\n",
    "    good_df, bad_df = process_partner(cfg)\n",
    "\n",
    "    # Keep in memory list for union into unified dataset\n",
    "    silver_dfs.append(good_df)\n",
    "\n",
    "    # Write rejects (if any)\n",
    "    if bad_df.limit(1).count() > 0:\n",
    "        bad_count = bad_df.count()\n",
    "        total_bad += bad_count\n",
    "        (bad_df.write.format(\"delta\").mode(\"append\").save(REJECTS_PATH))\n",
    "        print(f\"  - Wrote {bad_count} reject rows to {REJECTS_PATH}\")\n",
    "\n",
    "    good_count = good_df.count()\n",
    "    total_good += good_count\n",
    "    print(f\"  - Good rows: {good_count}\")\n",
    "\n",
    "# Unified silver dataset (single output)\n",
    "if not silver_dfs:\n",
    "    raise ValueError(\"No partner dataframes produced. Check input paths and configs.\")\n",
    "\n",
    "silver_df = silver_dfs[0]\n",
    "for df in silver_dfs[1:]:\n",
    "    silver_df = silver_df.unionByName(df)\n",
    "\n",
    "# Write standardized Silver (overwrite for the take-home; production could append/partition)\n",
    "(silver_df.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH))\n",
    "\n",
    "print(f\"\\n‚úÖ Silver written to: {SILVER_PATH}\")\n",
    "print(f\"‚úÖ Total good rows: {total_good}\")\n",
    "print(f\"‚úÖ Total bad rows:  {total_bad}\")\n",
    "\n",
    "display(spark.read.format(\"delta\").load(SILVER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a5d7ec-8677-43ca-87f6-bc65e16ab80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Publish Gold unified table (consumption)\n",
    "# =========================\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {GOLD_TABLE} (\n",
    "  external_id STRING,\n",
    "  first_name  STRING,\n",
    "  last_name   STRING,\n",
    "  dob         STRING,\n",
    "  email       STRING,\n",
    "  phone       STRING,\n",
    "  partner_code STRING\n",
    ")\n",
    "USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "(\n",
    "    spark.read.format(\"delta\").load(SILVER_PATH)\n",
    "         .select(STD_COLS)\n",
    "         .write.mode(\"overwrite\")\n",
    "         .saveAsTable(GOLD_TABLE)\n",
    ")\n",
    "\n",
    "print(f\"\\nüèÅ Gold table published: {GOLD_TABLE}\")\n",
    "spark.sql(f\"SELECT partner_code, COUNT(*) AS cnt FROM {GOLD_TABLE} GROUP BY partner_code\").show()\n",
    "spark.sql(f\"SELECT * FROM {GOLD_TABLE} ORDER BY partner_code, external_id\").show(truncate=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
